{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UXAjutbMhoa"
   },
   "source": [
    "#  Que peuvent faire les *transformers* ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3GIqRT-Mhoe"
   },
   "source": [
    "Installez la biblioth√®que ü§ó *Transformers* pour ex√©cuter ce *notebook*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KGUwAC1zMhoj"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers[sentencepiece]\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ehB754vsO78P"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__builtins__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__closure__',\n",
       " '__code__',\n",
       " '__defaults__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get__',\n",
       " '__getattribute__',\n",
       " '__globals__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__kwdefaults__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__name__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__qualname__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VmlNOR-OdIU"
   },
   "source": [
    "### Analyse de sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ILYepD-3Mhom"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598048329353333}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "uYmC3B-ZMhoo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.5260409116744995}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#classifier = pipeline(\"sentiment-analysis\", model=\"tblard/tf-allocine\")\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased\")\n",
    "classifier(\"J'ai attendu un cours d'HuggingFace toute ma vie.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSiD3eYEMhos"
   },
   "source": [
    "Int√©ressant ! On observe que le r√©sultat est n√©gatif l√† o√π pour la version en anglais le r√©sultat est positif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6jo4BXTXMhou"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.5260409116744995},\n",
       " {'label': 'LABEL_1', 'score': 0.5385863780975342}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\n",
    "    [\"J'ai attendu un cours d'HuggingFace toute ma vie.\", \n",
    "     \"Je d√©teste tellement √ßa !\"]\n",
    ") # pour classifier plusieurs phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpC8hlc_OBc8"
   },
   "source": [
    "La phrase \"J'ai attendu un cours d'HuggingFace toute ma vie.\" qui √©tait pr√©cedemment n√©gative devient √† pr√©sent positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LQ52fFqOrQa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgmwK2d6OlFE"
   },
   "source": [
    "### Z√©ro shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agGiUvz5Mho1"
   },
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"BaptisteDoyen/camembert-base-xnli\")\n",
    "classifier(\n",
    "    \"C'est un cours sur la biblioth√®que Transformers\",\n",
    "    candidate_labels=[\"√©ducation\", \"politique\", \"affaires\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1WCoo63Mho4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kujBeTwDO9_3"
   },
   "source": [
    "### G√©n√©ration de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "66Sc0NpBMho6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '# Dans ce cours, nous vous enseignerons comment nous pouvons √™tre des √™tres humains. \" \" Nous'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"asi/gpt-fr-cased-small\")\n",
    "generator(\"# Dans ce cours, nous vous enseignerons comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "wb6QpQF6Mho8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '# Dans ce cours, nous vous enseignerons comment nous pouvons √™tre des √™tres humains. \" \" Nous vous enseignerons comment nous pouvons √™tre des √™tres humains'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"asi/gpt-fr-cased-small\")\n",
    "generator(\n",
    "    \"# Dans ce cours, nous vous enseignerons comment\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvMBHts1Mho9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kU4aFbAPZ5a"
   },
   "source": [
    "### Remplacement des mots masqu√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6UV_20PfMho_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.10731782764196396,\n",
       "  'token': 15328,\n",
       "  'token_str': 'automobiles',\n",
       "  'sequence': 'Ce cours vous apprendra tout sur les mod√®les automobiles.'},\n",
       " {'score': 0.07520482689142227,\n",
       "  'token': 4007,\n",
       "  'token_str': '√©lectriques',\n",
       "  'sequence': 'Ce cours vous apprendra tout sur les mod√®les √©lectriques.'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\", model=\"camembert-base\")\n",
    "unmasker(\" Ce cours vous apprendra tout sur les mod√®les <mask>.\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyZdM9VqMhpC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvNljOY6Pao6"
   },
   "source": [
    "### Reconnaissance d'entit√©s nomm√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "_k7zh7FYMhpD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.60744476,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 12,\n",
       "  'end': 20},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.6868405,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 38,\n",
       "  'end': 51},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9952378,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 53,\n",
       "  'end': 62}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", model=\"Jean-Baptiste/camembert-ner\", grouped_entities=True)\n",
    "ner(\"Je m'appelle Sylvain et je travaille √† Hugging Face √† Brooklyn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZdeSFqnMhpF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEIu6FbnQUTb"
   },
   "source": [
    "### R√©ponse √† des questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ZTX2bUsgMhpG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.5418344736099243,\n",
       " 'start': 38,\n",
       " 'end': 63,\n",
       " 'answer': ' Hugging Face √† Brooklyn.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\", model=\"etalab-ia/camembert-base-squadFR-fquad-piaf\")\n",
    "question_answerer(\n",
    "    question=\"O√π est-ce que je travaille ?\",\n",
    "    context=\"Je m'appelle Sylvain et je travaille √† Hugging Face √† Brooklyn.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cz8QMHu8MhpI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZQEgjAZQW7-"
   },
   "source": [
    "###  R√©sum√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "7m_mQN_jMhpJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"Les universit√©s am√©ricaines dispensent d√©sormais des cours d'ing√©nierie centr√©s sur les sujets de l'infrastructure, l'environnement et les questions connexes.\"}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"moussaKam/barthez-orangesum-abstract\")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    L'Am√©rique a chang√© de fa√ßon spectaculaire au cours des derni√®res ann√©es. Non seulement le nombre de \n",
    "    dipl√¥m√©s dans les disciplines traditionnelles de l'ing√©nierie telles que le g√©nie m√©canique, civil, \n",
    "    l'√©lectricit√©, la chimie et l'a√©ronautique a diminu√©, mais dans la plupart \n",
    "    des grandes universit√©s am√©ricaines, les programmes d'√©tudes d'ing√©nierie se concentrent d√©sormais sur \n",
    "    et encouragent largement l'√©tude des sciences de l'ing√©nieur. Par cons√©quent, il y a \n",
    "    de moins en moins d'offres dans les sujets d'ing√©nierie traitant de l'infrastructure, \n",
    "    l'environnement et les questions connexes, et une plus grande concentration sur les sujets de haute \n",
    "    technologie, qui soutiennent en grande partie des d√©veloppements scientifiques de plus en plus \n",
    "    complexes. Si cette derni√®re est importante, elle ne doit pas se faire au d√©triment\n",
    "    de l'ing√©nierie plus traditionnelle.\n",
    "\n",
    "    Les √©conomies en d√©veloppement rapide telles que la Chine et l'Inde, ainsi que d'autres \n",
    "    pays industrialis√©s d'Europe et d'Asie, continuent d'encourager et de promouvoir\n",
    "    l'enseignement de l'ing√©nierie. La Chine et l'Inde, respectivement, dipl√¥ment \n",
    "    six et huit fois plus d'ing√©nieurs traditionnels que les √âtats-Unis. \n",
    "    Les autres pays industriels maintiennent au minimum leur production, tandis que l'Am√©rique \n",
    "    souffre d'une baisse de plus en plus importante du nombre de dipl√¥m√©s en ing√©nierie\n",
    "    et un manque d'ing√©nieurs bien form√©s.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6VvM9ZTMhpL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9Ombhj_QrDc"
   },
   "source": [
    "###  Traduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "VTIsplaDMhpM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Ce cours est produit par Hugging Face.'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "translator(\"This course is produced by Hugging Face.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
